# -*- coding: utf-8 -*-
"""BasicGym.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dxf8HdOK1Aj1peMQbc2IJpltFCfxkHtF

Hello everyone, and welcome to the class and its first discussion! Today, we are presenting a software tool you will likely use when implementing RL problems. This is the only practical discussion of the class, the others will be more theoretical. Today's tutorial is meant to give you an introduction to OpenAI Gym, which is used as a standard benchmark for reinforcement learning algorithms.

What is Gym exactly? Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and it is compatible with any numerical computation library, such as TensorFlow.

The library contains a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms and test them.

# Installation

Only requirement is to have Python 3.5+ installed on your computer. You can install it using the pip package manager or you can clone it from its github depository.
"""

import gym
import pybullet_envs
import ECE239AS_Envs

from sys import argv
from getopt import getopt

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

class OUActionNoise:
    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):
        self.theta = theta
        self.mean = mean
        self.std_dev = std_deviation
        self.dt = dt
        self.x_initial = x_initial
        self.reset()

    def __call__(self):
        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.
        x = (
            self.x_prev
            + self.theta * (self.mean - self.x_prev) * self.dt
            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)
        )
        # Store x into x_prev
        # Makes next noise dependent on current one
        self.x_prev = x
        return x

    def reset(self):
        if self.x_initial is not None:
            self.x_prev = self.x_initial
        else:
            self.x_prev = np.zeros_like(self.mean)

class Buffer:
    def __init__(self, buffer_capacity=100000, batch_size=64):
        # Number of "experiences" to store at max
        self.buffer_capacity = buffer_capacity
        # Num of tuples to train on.
        self.batch_size = batch_size

        # Its tells us num of times record() was called.
        self.buffer_counter = 0

        # Instead of list of tuples as the exp.replay concept go
        # We use different np.arrays for each tuple element
        self.state_buffer = np.zeros((self.buffer_capacity, num_states))
        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))
        self.reward_buffer = np.zeros((self.buffer_capacity, 1))
        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))
        self.done_buffer = np.zeros((self.buffer_capacity, 1))

    # Takes (s,a,r,s') obervation tuple as input
    def record(self, obs_tuple):
        # Set index to zero if buffer_capacity is exceeded,
        # replacing old records
        index = self.buffer_counter % self.buffer_capacity

        self.state_buffer[index] = obs_tuple[0]
        self.action_buffer[index] = obs_tuple[1]
        self.reward_buffer[index] = obs_tuple[2]
        self.next_state_buffer[index] = obs_tuple[3]
        self.done_buffer[index] = obs_tuple[4]

        self.buffer_counter += 1

    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows
    # TensorFlow to build a static graph out of the logic and computations in our function.
    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.
    @tf.function
    def update(
        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, update_actor
    ):
        # Training and updating Actor & Critic networks.
        # See Pseudo Code.
        target_actions = target_actor(next_state_batch, training=True)
        if TD3: target_actions += tf.random.normal(target_actions.shape, stddev=0.0) #05) #0.01)

        # Use minimum of two target networks to calculate Q-value targets
        eval = lambda critic: reward_batch + done_batch * gamma * critic(
            [next_state_batch, target_actions], training=True
        )
        y = eval(target_critic)
        if TD3: y = tf.math.minimum(y, eval(target_critic2))

        # Regress critic_model toward targets
        with tf.GradientTape() as tape:
            critic_value = critic_model([state_batch, action_batch], training=True)
            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))
        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)
        critic_optimizer.apply_gradients(
            zip(critic_grad, critic_model.trainable_variables)
        )
        # Regress critic2_model toward targets
        if TD3:
          with tf.GradientTape() as tape:
              critic2_value = critic2_model([state_batch, action_batch], training=True) if TD3 else None
              critic2_loss = tf.math.reduce_mean(tf.math.square(y - critic2_value)) if TD3 else None
          critic2_grad = tape.gradient(critic2_loss, critic2_model.trainable_variables)
          critic2_optimizer.apply_gradients(
              zip(critic2_grad, critic2_model.trainable_variables)
          )

        # Skip remainder (DO NOT update actor)
        if not update_actor: return

        # TD3-NOTE: Still use critic_model (1) to optimize policy.
        with tf.GradientTape() as tape:
            actions = actor_model(state_batch, training=True)
            critic_value = critic_model([state_batch, actions], training=True)
            # Used `-value` as we want to maximize the value given
            # by the critic for our actions
            actor_loss = -tf.math.reduce_mean(critic_value)

        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)
        actor_optimizer.apply_gradients(
            zip(actor_grad, actor_model.trainable_variables)
        )

    # We compute the loss and update parameters
    def learn(self, update_actor=True):
        # Get sampling range
        record_range = min(self.buffer_counter, self.buffer_capacity)
        # Randomly sample indices
        batch_indices = np.random.choice(record_range, self.batch_size)

        # Convert to tensors
        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])
        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])
        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])
        reward_batch = tf.cast(reward_batch, dtype=tf.float32)
        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])
        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])
        done_batch = tf.cast(done_batch, dtype=tf.float32)

        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, update_actor)


# This update target parameters slowly
# Based on rate `tau`, which is much less than one.
@tf.function
def update_target(target_weights, weights, tau):
    for (a, b) in zip(target_weights, weights):
        a.assign(b * tau + a * (1 - tau))

def get_actor():
    # Initialize weights between -3e-3 and 3-e3
    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)

    inputs = layers.Input(shape=(num_states,))
    out = layers.Dense(ActorNN, activation="relu")(inputs)
    out = layers.Dense(ActorNN, activation="relu")(out)
    outputs = layers.Dense(1, activation="tanh", kernel_initializer=last_init)(out)

    # Our upper bound is 2.0 for Pendulum.
    outputs = outputs * upper_bound
    model = tf.keras.Model(inputs, outputs)
    return model

def get_critic():
    # State as input
    state_input = layers.Input(shape=(num_states))
    state_out = layers.Dense(16, activation="relu")(state_input)
    state_out = layers.Dense(32, activation="relu")(state_out)

    # Action as input
    action_input = layers.Input(shape=(num_actions))
    action_out = layers.Dense(32, activation="relu")(action_input)

    # Both are passed through seperate layer before concatenating
    concat = layers.Concatenate()([state_out, action_out])

    out = layers.Dense(CriticNN, activation="relu")(concat)
    out = layers.Dense(CriticNN, activation="relu")(out)
    outputs = layers.Dense(1)(out)

    # Outputs single value for give state-action
    model = tf.keras.Model([state_input, action_input], outputs)

    return model

def policy(state, noise_object):
    sampled_actions = tf.squeeze(actor_model(state))
    noise = noise_object()
    # Adding noise to action
    sampled_actions = sampled_actions.numpy() + noise

    # We make sure action is within bounds
    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)

    return [np.squeeze(legal_action)]

envs_pyb = ["InvertedPendulumBulletEnv-v0",
            "CartPoleContinuousBulletEnv-v0",
            "CartPoleWobbleContinuousEnv-v0"]
# problem = "Pendulum-v0"
# problem = "MountainCarContinuous-v0"
# problem = "Acrobot-v1"
problem = envs_pyb[2]
env = gym.make(problem)

num_states = env.observation_space.shape[0]
print("Size of State Space ->  {}".format(num_states))
num_actions = env.action_space.shape[0]
print("Size of Action Space ->  {}".format(num_actions))

upper_bound = +1.0 #env.action_space.high[0]
lower_bound = -1.0 #env.action_space.low[0]

print("Max Value of Action ->  {}".format(upper_bound))
print("Min Value of Action ->  {}".format(lower_bound))

opt, args = getopt(argv[1:], "", ["TD3", "ActorNN=", "CriticNN="])
opt = dict(opt)

TD3 = "--TD3" in opt
PartTD3 = True
ActorNN = int(opt.get('--ActorNN',32))
CriticNN = int(opt.get('--CriticNN',32))

std_dev = 0.5 #1.5
min_std_dev = 0.01
ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))

model_path = None #"TD3-CartPoleWobbleContinuousEnv-v0_No9"

actor_model = get_actor()
critic_model = get_critic()
critic2_model = get_critic() if TD3 else None

if model_path:
    print(f'Loading old models from models/{model_path}')
    actor_model, critic_model, critic2_model = [
        tf.keras.models.load_model(f'models/{model_path}/actor'),
        tf.keras.models.load_model(f'models/{model_path}/critic'),
        tf.keras.models.load_model(f'models/{model_path}/critic2') if TD3 else None
    ]
target_actor = get_actor()
target_critic = get_critic()
target_critic2 = get_critic() if TD3 else None

# Making the weights equal initially
target_actor.set_weights(actor_model.get_weights())
target_critic.set_weights(critic_model.get_weights())
if TD3: target_critic2.set_weights(critic2_model.get_weights())

# Learning rate for actor-critic models
critic_lr = 0.001
actor_lr = 0.001

critic_optimizer = tf.keras.optimizers.Adam(critic_lr)
critic2_optimizer = tf.keras.optimizers.Adam(critic_lr) if TD3 else None
actor_optimizer = tf.keras.optimizers.Adam(actor_lr)

total_episodes = 1000
# Discount factor for future rewards
gamma = 0.99
# Used to update target networks
tau = 0.002  # Original: 0.005

buffer = Buffer(500000, 64)

# To store reward history of each episode
ep_reward_list = []
# To store average reward history of last few episodes
avg_reward_list = []

output_csv = [["ActorNN",ActorNN,"CriticNN",CriticNN]]
output_csv.append(["Ep","Reward","AvgReward40"])
try:
    # Takes about 4 min to train
    for ep in range(total_episodes):

        if problem in envs_pyb: env.render()
        prev_state = env.reset()
        episodic_reward = 0

        moves = []
        for step in range(10000000):
            # Uncomment this to see the Actor in action
            # But not in a python notebook.
            if ep % 5 == 0: env.render()
            # env.render()

            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)

            action = policy(tf_prev_state, ou_noise)
            moves.append(action)
            # Recieve state and reward from environment.
            state, reward, done, info = env.step(action)
            # reward += 10*np.abs(action)

            buffer.record((prev_state, action, reward, state, 0.0 if done else 1.0))
            episodic_reward += reward

            update_actor_and_targets = not TD3 or step % 1 == 0
            buffer.learn(update_actor_and_targets)

            # Update target networks every other step
            if update_actor_and_targets:
              update_target(target_actor.variables, actor_model.variables, tau)
              update_target(target_critic.variables, critic_model.variables, tau)
              if TD3: update_target(target_critic2.variables, critic2_model.variables, tau)

            # End this episode when `done` is True
            if done:
                break

            prev_state = state

        # print("Single episode: ", episodic_reward)
        ep_reward_list.append(episodic_reward)

        # Mean of last 40 episodes
        avg_reward = np.mean(ep_reward_list[-40:])
        print(round(episodic_reward,2), ":", np.round(ou_noise.std_dev,2), "Ep * {} * AvgR = {}. AvgMove {} with mag {}".
                                    format(ep, avg_reward, round(np.mean(moves),2), round(np.mean(np.abs(moves)),2)))
        avg_reward_list.append(avg_reward)
        output_csv.append([ep, round(episodic_reward,2), round(avg_reward,2)])

        # Decrease noise
        ou_noise.std_dev = np.maximum(min_std_dev, ou_noise.std_dev * 0.98)

except KeyboardInterrupt:
    pass

import os
model_name = f'models/{"TD3" if TD3 else "DDPG"}-{problem}'
print(f'Early termination, saving as {model_name}')
i = 1
itered = model_name
while os.path.isdir(itered):
    itered = f'{model_name}_No{i}'
    i += 1
model_name = itered
os.mkdir(f'{model_name}')
with open(f'{model_name}/{round(np.max(avg_reward_list),2)}', 'w') as f:
    for arr in output_csv:
        print(*arr, sep=', ', file=f)

models = {
    'actor': actor_model,
    'critic': critic_model
}
if TD3: models['critic2'] = critic2_model

for model in models:
    path = f'{model_name}/{model}'
    print('Saving', model, 'to', path)
    models[model].save(path)
